{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C3EbsNtVRUj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packets and libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Input, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import MobileNetV2,ResNet50\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "j2HsAPQSVVV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "_B5foGEKVdZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 256\n",
        "EPOCHS = 50\n",
        "BATCH = 10\n",
        "LR = 1e-4\n",
        "\n",
        "PATH = \"/content/drive/MyDrive//OTU-2D-Dataset-main/OTU-2D-Dataset-main/dataset_split\""
      ],
      "metadata": {
        "id": "BwuWvDi1VfDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "def load_data(path):\n",
        "    def load_split(split):\n",
        "        images = sorted(glob(os.path.join(path, split, 'images', '*')))\n",
        "        masks = sorted(glob(os.path.join(path, split, 'masks', '*')))\n",
        "        return images, masks\n",
        "\n",
        "    train_x, train_y = load_split('train')\n",
        "    val_x, val_y = load_split('validation')\n",
        "    test_x, test_y = load_split('test')\n",
        "\n",
        "    return (train_x, train_y), (val_x, val_y), (test_x, test_y)\n"
      ],
      "metadata": {
        "id": "I4Wqw6DXVgEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image(path):\n",
        "    if isinstance(path, bytes):\n",
        "        path = path.decode('utf-8')\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    if x is None:\n",
        "        raise ValueError(f\"Could not read image: {path}\")\n",
        "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
        "    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    x = x.astype(np.float32) / 255.0\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    if isinstance(path, bytes):\n",
        "        path = path.decode('utf-8')\n",
        "    y = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    if y is None:\n",
        "        raise ValueError(f\"Could not read mask: {path}\")\n",
        "    y = cv2.resize(y, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    y = y.astype(np.float32) / 255.0\n",
        "    _, y = cv2.threshold(y, 0, 1, cv2.THRESH_BINARY)\n",
        "    y = np.expand_dims(y, axis=-1)\n",
        "    return y\n",
        "\n",
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
        "    y.set_shape([IMAGE_SIZE, IMAGE_SIZE, 1])\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset(x, y, batch=8):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.repeat()\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "VUsa6q6GVma7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(PATH)\n",
        "\n",
        "print(\"Training data: \", len(train_y))\n",
        "print(\"Validation data: \", len(valid_x))\n",
        "print(\"Testing data: \", len(test_x))"
      ],
      "metadata": {
        "id": "fKm_sgu2Vot8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def show_random_image_with_mask(x_paths, y_paths, index=None):\n",
        "    if index is None:\n",
        "        index = random.randint(0, len(x_paths) - 1)\n",
        "\n",
        "    image = read_image(x_paths[index])\n",
        "    mask = read_mask(y_paths[index])\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(\"Ảnh gốc\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(mask.squeeze(), cmap='gray')\n",
        "    plt.title(\"Mask nhị phân\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "s41UcSxbV1uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_image_with_mask(train_x, train_y)\n"
      ],
      "metadata": {
        "id": "xuNq2_cpV3Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf_dataset(train_x, train_y, batch=BATCH)\n",
        "valid_dataset = tf_dataset(valid_x, valid_y, batch=BATCH)\n",
        "test_dataset = tf_dataset(test_x, test_y, batch=BATCH)"
      ],
      "metadata": {
        "id": "rR5cnqomV5QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "\n",
        "# Constants\n",
        "beta = 0.25\n",
        "alpha = 0.26\n",
        "gamma = 2.3\n",
        "epsilon = 1e-7\n",
        "smooth = 1e-15\n",
        "\n",
        "# Dice Coefficient\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + epsilon) / (\n",
        "            tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + epsilon)\n",
        "\n",
        "# Sensitivity\n",
        "def sensitivity(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + epsilon)\n",
        "\n",
        "# Specificity\n",
        "def specificity(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "    true_negatives = tf.reduce_sum(tf.round(tf.clip_by_value((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    possible_negatives = tf.reduce_sum(tf.round(tf.clip_by_value(1 - y_true, 0, 1)))\n",
        "    return true_negatives / (possible_negatives + epsilon)\n",
        "\n",
        "# Convert probabilities to logits\n",
        "def convert_to_logits(y_pred):\n",
        "    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "    return tf.math.log(y_pred / (1 - y_pred))\n",
        "\n",
        "# Weighted cross entropy\n",
        "def weighted_cross_entropyloss(y_true, y_pred):\n",
        "    y_pred = convert_to_logits(y_pred)\n",
        "    pos_weight = beta / (1 - beta)\n",
        "    loss = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, labels=y_true, pos_weight=pos_weight)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "# Dice Loss\n",
        "def generalized_dice_coefficient(y_true, y_pred):\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1 - generalized_dice_coefficient(y_true, y_pred)\n",
        "\n",
        "# Confusion metrics\n",
        "def confusion(y_true, y_pred):\n",
        "    y_pred = tf.cast(tf.clip_by_value(y_pred, 0, 1), tf.float32)\n",
        "    y_true = tf.cast(tf.clip_by_value(y_true, 0, 1), tf.float32)\n",
        "    y_pred_neg = 1 - y_pred\n",
        "    y_neg = 1 - y_true\n",
        "    tp = tf.reduce_sum(y_true * y_pred)\n",
        "    fp = tf.reduce_sum(y_neg * y_pred)\n",
        "    fn = tf.reduce_sum(y_true * y_pred_neg)\n",
        "    precision = (tp + smooth) / (tp + fp + smooth)\n",
        "    recall = (tp + smooth) / (tp + fn + smooth)\n",
        "    return precision, recall\n",
        "\n",
        "# True positive\n",
        "def true_positive(y_true, y_pred):\n",
        "    y_pred = tf.round(tf.clip_by_value(y_pred, 0, 1))\n",
        "    y_true = tf.round(tf.clip_by_value(y_true, 0, 1))\n",
        "    return (tf.reduce_sum(y_true * y_pred) + smooth) / (tf.reduce_sum(y_true) + smooth)\n",
        "\n",
        "# True negative\n",
        "def true_negative(y_true, y_pred):\n",
        "    y_pred = tf.round(tf.clip_by_value(y_pred, 0, 1))\n",
        "    y_true = tf.round(tf.clip_by_value(y_true, 0, 1))\n",
        "    y_pred_neg = 1 - y_pred\n",
        "    y_neg = 1 - y_true\n",
        "    return (tf.reduce_sum(y_neg * y_pred_neg) + smooth) / (tf.reduce_sum(y_neg) + smooth)\n",
        "\n",
        "# Jaccard\n",
        "def jaccard_similarity(y_true, y_pred):\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    union = tf.reduce_sum(y_true_f + y_pred_f - y_true_f * y_pred_f)\n",
        "    return intersection / (union + epsilon)\n",
        "\n",
        "def jaccard_loss(y_true, y_pred):\n",
        "    return 1 - jaccard_similarity(y_true, y_pred)\n",
        "\n",
        "# Binary Dice\n",
        "def binary_dice(y_true, y_pred):\n",
        "    return 0.5 * binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "yXl7Y2nFV8Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import normalize\n",
        "from tqdm import tqdm\n",
        "\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers as L\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Convolutional block\n",
        "def conv_block(x, num_filters):\n",
        "    x = L.Conv2D(num_filters, 3, padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(x)\n",
        "    x = L.Conv2D(num_filters, 3, padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "# Encoder block\n",
        "def encoder_block(x, num_filters):\n",
        "    x = conv_block(x, num_filters)\n",
        "    p = L.MaxPool2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "# Decoder block\n",
        "def decoder_block(x, s, num_filters):\n",
        "    x = L.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(x)\n",
        "    x = L.Concatenate()([x, s])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "# Build U-Net model\n",
        "def build_unet(input_shape):\n",
        "    inputs = L.Input(input_shape)\n",
        "\n",
        "    c1, p1 = encoder_block(inputs, 64)\n",
        "    c2, p2 = encoder_block(p1, 128)\n",
        "    c3, p3 = encoder_block(p2, 256)\n",
        "    c4, p4 = encoder_block(p3, 512)\n",
        "\n",
        "    c5 = conv_block(p4, 1024)\n",
        "\n",
        "    d6 = decoder_block(c5, c4, 512)\n",
        "    d7 = decoder_block(d6, c3, 256)\n",
        "    d8 = decoder_block(d7, c2, 128)\n",
        "    d9 = decoder_block(d8, c1, 64)\n",
        "\n",
        "    outputs = L.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d9)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "DPWmmd1rWBPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " input = (256, 256, 3)\n",
        "model = build_unet(input)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "MaC-l8ZIWFDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_path = \"unet_with_original_images.h5\"\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    monitor='val_loss',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    mode='auto',\n",
        "    save_freq='epoch'\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False),\n",
        "    checkpoint_callback\n",
        "]"
      ],
      "metadata": {
        "id": "NZ5zgFJGWKvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_steps = len(train_x) // BATCH\n",
        "valid_steps = len(valid_x) // BATCH\n",
        "\n",
        "if len(train_x) % BATCH != 0:\n",
        "    train_steps += 1\n",
        "if len(valid_x) % BATCH != 0:\n",
        "    valid_steps += 1\n",
        "\n",
        "print(train_steps)\n",
        "print(valid_steps)"
      ],
      "metadata": {
        "id": "TE3wxuLhWPOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "opt = tf.keras.optimizers.Nadam(LR,clipnorm=1.0)\n",
        "metrics = [dice_coef, jaccard_similarity, tf.keras.metrics.Recall(), tf.keras.metrics.Precision()]\n",
        "\n",
        "model.compile(loss=dice_loss, optimizer=opt, metrics=metrics)"
      ],
      "metadata": {
        "id": "SmwXrH7yWRyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_steps=valid_steps,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "id": "oFwiYVCqWTPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"/content/drive/MyDrive/unet_with_original_images_weights.weights.h5\")\n"
      ],
      "metadata": {
        "id": "h1-cgNwsWYPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = tf_dataset(test_x, test_y, batch=BATCH)\n",
        "\n",
        "test_steps = (len(test_x)//BATCH)\n",
        "if len(test_x) % BATCH != 0:\n",
        "    test_steps += 1\n",
        "model.evaluate(test_dataset, steps=test_steps)"
      ],
      "metadata": {
        "id": "b29MiX4tWbhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def predict_and_save(model, image_path, mask_path, output_dir):\n",
        "    # Đọc ảnh\n",
        "    img = read_image(image_path.encode('utf-8'))\n",
        "    true_mask = read_mask(mask_path.encode('utf-8'))\n",
        "\n",
        "    img_input = np.expand_dims(img, axis=0)\n",
        "    pred_mask = model.predict(img_input)[0]\n",
        "\n",
        "\n",
        "    name = os.path.basename(image_path).split('.')[0]\n",
        "    out_path = os.path.join(output_dir, name)\n",
        "    os.makedirs(out_path, exist_ok=True)\n",
        "\n",
        "    # Lưu ảnh gốc\n",
        "    img_save = (img * 255).astype(np.uint8)\n",
        "    img_save = cv2.cvtColor(img_save, cv2.COLOR_RGB2BGR)\n",
        "    cv2.imwrite(os.path.join(out_path, f\"{name}_image.jpg\"), img_save)\n",
        "\n",
        "    # Lưu mask thật\n",
        "    mask_save = (true_mask * 255).astype(np.uint8)\n",
        "    cv2.imwrite(os.path.join(out_path, f\"{name}_true_mask.png\"), mask_save)\n",
        "\n",
        "    # Lưu mask dự đoán\n",
        "    pred_mask_save = (pred_mask * 255).astype(np.uint8)\n",
        "    cv2.imwrite(os.path.join(out_path, f\"{name}_pred_mask.png\"), pred_mask_save)\n",
        "\n",
        "def process_predictions(model, data, output_dir):\n",
        "    x_data, y_data = data\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for x_path, y_path in tqdm(zip(x_data, y_data), total=len(x_data)):\n",
        "        predict_and_save(model, x_path, y_path, output_dir)\n"
      ],
      "metadata": {
        "id": "mfyxPfzmWj5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"/content/drive/MyDrive//OTU-2D-Dataset-main/OTU-2D-Dataset-main/dataset_split/predictions\"\n",
        "process_predictions(model, (test_x, test_y), output_dir)\n"
      ],
      "metadata": {
        "id": "uw4VywPjWleQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}